{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1826bd8f",
   "metadata": {},
   "source": [
    "# Pre-Lecture HW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2e18e8",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef54773",
   "metadata": {},
   "source": [
    "1. Simple Linear Regression uses a single predictor variable to predict a continuous outcome variable, while Multiple Linear Regression utilizes multiple predictor variables to predict a continuous outcome variable. Multiple Linear Regression offers several advantages over Simple Linear Regression, including the ability to model complex relationships between predictor and outcome variables, providing a more realistic representation of real-world phenomena, and potentially improving predictive accuracy. However, it also introduces the challenge of multicollinearity, where highly correlated predictor variables can make it difficult to determine the individual contribution of each predictor to the outcome variable. This can lead to unreliable estimation of coefficient parameters and reduced generalizability of the fitted model. While multicollinearity can be problematic for statistical inference and interpretability, it may not hinder predictive generalizability if the primary goal is prediction rather than understanding the specific effects of each predictor variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5893d9",
   "metadata": {},
   "source": [
    "2. Simple Linear Regression is a statistical method used to analyze the relationship between two variables. A continuous variable, such as height, weight, or temperature, is a numerical value that represents the average change in the outcome variable for a single-unit increase in the predictor variable. In the linear form of the model, the continuous variable is multiplied by the slope coefficient, representing the average change in the outcome variable for a single-unit increase in the predictor variable. An indicator variable, also known as a dummy variable, is a binary variable that takes on the value of 1 if a specific condition is met and 0 otherwise. It is used to represent categorical data, such as gender, ethnicity, or treatment group, in the regression model. The coefficient of an indicator variable in the linear form represents the average difference in the outcome variable between the two groups represented by the indicator variable. Using an indicator variable in simple linear regression allows for comparison of the average difference in the outcome variable between two groups, which can be useful for evaluating the effect of a treatment or intervention or understanding how the outcome variable varies across different categories of a categorical predictor variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289a06fd",
   "metadata": {},
   "source": [
    "3. In a Multiple Linear Regression model, the behavior changes from modeling a single line to modeling two parallel lines. This is because the indicator variable creates two distinct groups, and the model estimates a separate intercept for each group, shifting the line vertically for one group while maintaining the same slope for both groups. The Simple Linear Regression model with a continuous predictor estimates a single line representing the relationship between the continuous predictor and the outcome variable. The Multiple Linear Regression model estimates two parallel lines, one for each group defined by the indicator variable. The β1 coefficient represents the contrast or difference in the intercept between the two groups, shifting one line vertically relative to the other. The β2 coefficient represents the slope, indicating a consistent relationship between the continuous predictor and the outcome variable across both groups. Incorporating an indicator variable in a Simple Linear Regression model fundamentally changes how the model represents data, moving from representing the relationship between the predictor and outcome with a single line to using two parallel lines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7313b3",
   "metadata": {},
   "source": [
    "4. In a Multiple Linear Regression model, an interaction term between a continuous and an indicator variable allows the slope of the relationship between the continuous predictor and the outcome variable to vary across the groups defined by the indicator variable. This allows the lines representing the relationship to have different slopes and intercepts, reflecting potential differences in how the continuous predictor influences the outcome across the groups. The linear form of a Multiple Linear Regression model with an interaction between a continuous and an indicator predictor is: \n",
    "\n",
    "outcome = β0 + β1 * 1(predictorindicator) + β2 * predictorcontinuous + β3 * 1(predictorindicator) * predictorcontinuous.\n",
    "\n",
    "This model allows for a more nuanced understanding of the relationship between the continuous and indicator variables, as the interaction term allows the relationship between the continuous predictor and the outcome variable to fundamentally change across groups. This can capture scenarios where the effect of the continuous predictor is enhanced or diminished in the presence of the condition represented by the indicator variable. For example, a model predicting salary based on years of experience and gender would have an interaction term between experience and gender, allowing for the possibility that the relationship between experience and salary differs for men and women. This provides a more accurate and insightful model of the complex relationship between these variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e788f5",
   "metadata": {},
   "source": [
    "5. A Multiple Linear Regression (MLR) model is a statistical method that uses indicator variables derived from a non-binary categorical variable to model the average differences in the outcome variable across multiple groups. This is achieved by using a set of binary indicator variables, where each indicator represents one level of the categorical variable, except for a designated \"baseline\" group. The coefficients of these indicator variables represent the average difference in the outcome variable between each group and the baseline group. For example, if the categorical variable is \"Region\" with levels \"East,\" \"West,\" \"North,\" and \"South,\" and \"East\" is chosen as the baseline, the MLR model would include three indicator variables: β0, β1 * 1, β2 * 1, and β3 * 1. The linear form of this model is:\n",
    "\n",
    "outcome = β0 + β1 * 1(Region = \"West\") + β2 * 1(Region = \"North\") + β3 * 1(Region = \"South\")\n",
    "\n",
    "The model assumes that the relationship between the categorical variable and the outcome is captured entirely by these average differences, and does not model any interaction between the levels of the categorical variable or any other predictor variables. The choice of the baseline group is arbitrary and does not affect the overall fit of the model. Binary variable encodings are used to represent the non-binary categorical variable, allowing the MLR model to estimate the unique effect of each category level on the outcome variable by comparing it to the chosen baseline group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbc5590",
   "metadata": {},
   "source": [
    "Summary:\n",
    "\n",
    "The conversation explored the differences between using continuous and indicator variables in simple linear regression and how introducing indicator variables and interactions impacts the behavior of multiple linear regression models.\n",
    "\n",
    "● Continuous vs. Indicator Variables: A continuous variable in Simple Linear Regression (SLR) takes on a range of numerical values, while an indicator variable is binary (0 or 1), representing categorical data. The linear form of SLR differs depending on the type of predictor variable. The benefit of using an indicator variable is the ability to compare average differences in the outcome variable between groups.\n",
    "\n",
    "● Introducing an Indicator Variable to SLR: When an indicator variable is added alongside a continuous variable in a Multiple Linear Regression (MLR) model, the model shifts from representing the data with a single line to using two parallel lines. This allows the model to account for average differences in the outcome between the groups defined by the indicator variable while maintaining a consistent relationship between the continuous predictor and the outcome.\n",
    "\n",
    "● Interactions in MLR: Adding an interaction term between a continuous and an indicator variable in MLR allows for varying slopes across groups, meaning the lines are no longer constrained to be parallel. This captures scenarios where the continuous predictor's effect changes depending on the group defined by the indicator variable.\n",
    "\n",
    "● Non-Binary Categorical Variables in MLR: An MLR model with only indicator variables from a non-binary categorical variable models average differences in the outcome across multiple groups. This requires creating multiple binary indicator variables, each representing one level of the categorical variable (except the baseline group). The coefficients represent the average difference in the outcome between each group and the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419a7309",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c154c079",
   "metadata": {},
   "source": [
    "In this scenario, the outcome variable is sales, which we can represent as $Y_i$. The predictor variables are the amount spent on TV advertising (x<sub>1i</sub>) and the amount spent on online advertising (x<sub>2i</sub>), both of which are continuous variables. A meaningful interaction to consider is whether the effectiveness of TV advertising changes depending on the amount spent on online advertising, and vice versa. This means we need to consider if the relationship between one predictor and the outcome (sales) is influenced by the value of the other predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee1b7fa",
   "metadata": {},
   "source": [
    "Linear Forms:\n",
    "\n",
    "Without Interaction: sales = $\\beta_0$ + $\\beta_1$ TV advertising + $\\beta_2$ online advertising\n",
    "\n",
    "With Interaction: sales = $\\beta_0$ + $\\beta_1$ TV advertising + $\\beta_2$ online advertising + $\\beta_3$ (TV advertising $\\times$ online advertising)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525a813b",
   "metadata": {},
   "source": [
    "To make predictions using these formulas, you would plug in the values for TV advertising and online advertising and calculate the resulting sales value. The interaction-free approach presumes that the impact of every advertising media is unrelated to the others. The model with interaction takes into account the potential for spending on one advertising channel to either increase or decrease the impact of that medium. \n",
    "\n",
    "If there were two categories for advertising spending, \"high\" and \"low,\" you would make binary indicator variables for each:\n",
    "\n",
    "● High TV advertising: 1 indicates high TV advertising, 0 otherwise\n",
    "\n",
    "● High online advertising: 1 denotes high online advertising, 0 otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c5b843",
   "metadata": {},
   "source": [
    "The linear forms would then be updated accordingly:\n",
    "Without Interaction:\n",
    "\n",
    "● sales = $\\beta_0$ + $\\beta_1$ 1(TV advertising high) + $\\beta_2$ 1(online advertising high)\n",
    "With Interaction:\n",
    "\n",
    "● sales = $\\beta_0$ + $\\beta_1$ 1(TV advertising high) + $\\beta_2$ 1(online advertising high) + $\\beta_3$ (1(TV advertising high) $\\times$ 1(online advertising high))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c502243",
   "metadata": {},
   "source": [
    "Summary:\n",
    "\n",
    "The scenario involves a sports equipment company's advertising campaign, with the outcome variable being sales, represented as Yi, and the predictor variables being TV advertising spending and online advertising spending. A meaningful interaction exists if the effectiveness of TV advertising changes depending on the online advertising spending, and vice versa. Linear forms can be either without or with interaction, depending on the potential synergistic effect.\n",
    "\n",
    "Continuous and categorical predictors are used, with linear forms for each type of advertising medium. The model without interaction assumes independence between advertising mediums, while the model with interaction allows for amplified or diminished effects due to spending on the other medium.\n",
    "\n",
    "Interactions are crucial, as they indicate that the effect of one predictor on the outcome depends on the level of another. The interaction term in the model captures the potential synergistic effect of TV and online advertising.\n",
    "\n",
    "Interactions can be additive or synergistic, with additive interactions implying the combined effect of two predictors is simply the sum of their individual effects, and synergistic interactions implying the combined effect is greater than the sum of individual effects. The choice between models depends on the research question and the evidence in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e30a16",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c87baec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_70/1424844215.py:5: DtypeWarning: Columns (408,1001,1002,1006,1007,1008,1080,1113,1115,1116,1117,1118,1119,1120,1121,1124,1125,1126,1127,1128,1213,1214,1215,1216,1217,1218,1342,1343,1344,1345,1346,1347,1348,1349,1390,1391,1393,1463,1549,1552,1555,1558,1561) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cscs = pd.read_csv('https://raw.githubusercontent.com/pointOfive/stat130chat130/refs/heads/main/CP/CSCS_data_anon.csv')\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Loneliness_Score'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3789\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3791\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Loneliness_Score'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m cscs \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/pointOfive/stat130chat130/refs/heads/main/CP/CSCS_data_anon.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Create a binary outcome variable (example)\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m cscs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeels_lonely\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mcscs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLoneliness_Score\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Replace 'loneliness_scale' and 'threshold' with actual column name and threshold value\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Specify the linear form (example)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m smf\u001b[38;5;241m.\u001b[39mlogit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeels_lonely ~ age + Social_Support_Score\u001b[39m\u001b[38;5;124m'\u001b[39m, data\u001b[38;5;241m=\u001b[39mcscs)\u001b[38;5;241m.\u001b[39mfit()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3793\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3794\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3795\u001b[0m     ):\n\u001b[1;32m   3796\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3797\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3798\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3799\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3800\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Loneliness_Score'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Load the CSCS dataset\n",
    "cscs = pd.read_csv('https://raw.githubusercontent.com/pointOfive/stat130chat130/refs/heads/main/CP/CSCS_data_anon.csv')\n",
    "\n",
    "# Create a binary outcome variable (example)\n",
    "cscs['feels_lonely'] = cscs['Loneliness_Score'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "# Replace 'loneliness_scale' and 'threshold' with actual column name and threshold value\n",
    "\n",
    "# Specify the linear form (example)\n",
    "model = smf.logit('feels_lonely ~ age + Social_Support_Score', data=cscs).fit()\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2457857",
   "metadata": {},
   "source": [
    "Summary:\n",
    "\n",
    "You began by asking about using logistic regression or multi-class classification for your analysis of the Canadian Social Connection Survey (CSCS) dataset, as an alternative to multiple linear regression. The sources primarily focus on multiple linear regression examples, but they do provide some information on other regression techniques, including logistic regression for binary outcome variables.\n",
    "The choice between these techniques, as explained in the sources, depends on the nature of your outcome variable. Multiple linear regression is appropriate for continuous numeric outcomes, while logistic regression is used for binary (two-category) outcomes, and multi-class classification is employed when the outcome has more than two categories.\n",
    "The sources provide an example of using logistic regression to predict a binary variable (str8fyre) derived from a categorical variable (Type 1) in the Poke the Man dataset. This example highlights the requirement that the outcome variable in smf.logit must be binary (0 or 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48b39e4",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0752fc",
   "metadata": {},
   "source": [
    "Both the overall explanatory power of the model and the statistical significance of each predictor variable should be taken into account when assessing a multiple linear regression model. Despite their similarities, these ideas provide distinct perspectives on how well the model works and how the variables connect to one another. R-squared represents the proportion of variability in the outcome variable that is explained by the predictor variables in the model. A low R<sup>2</sup>, such as 17.6%, indicates that the model doesn't account for a large portion of the variation in the outcome. On the other hand, Coefficient Hypothesis Testing and p-values assess the evidence against the null hypothesis of \"no association\" between a specific predictor variable and the outcome. A low p-value (typically below a predetermined significance level, like 0.05) provides strong evidence against the null hypothesis, suggesting that the predictor variable likely has a statistically significant relationship with the outcome.\n",
    "\n",
    "It's possible to have a model with a low R<sup>2</sup> and yet still have statistically significant coefficients for some predictor variables. This scenario can occur due to several factors. Firstly, you could have weak predictors, where the predictor variables, even if statistically significant, might only weakly influence the outcome. Their combined effect might not be strong enough to explain a large portion of the variability in the outcome, resulting in a low R<sup>2</sup>. You could also have multicollinearity, caused by predictor variables that are highly correlated, which can make it challenging to isolate the individual effect of each predictor on the outcome. This can lead to inflated standard errors and less precise coefficient estimates, making it harder to detect strong evidence against the null hypothesis, despite the presence of a real relationship. However, multicollinearity doesn't necessarily invalidate the model's predictive power, as long as the combined effect of the collinear variables is meaningful for prediction. Finally, you could have missing predictors, which happen whenthe model is missing crucial predictor variables that could substantially improve its explanatory power.\n",
    "\n",
    "Essentially, the contradiction occurs because the significance of coefficients concentrates on the individual contributions of predictor variables, whereas R2 represents the model's overall explanatory ability. Statistically significant predictors in a model may exhibit an influence when taken separately, but they may not account for a large portion of the variability in the results when taken as a whole."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da65652e",
   "metadata": {},
   "source": [
    "Summary:\n",
    "\n",
    "This interaction explores the seeming contradiction between a multiple linear regression model having a low R<sup>2</sup> value, indicating it explains only a small portion of the variability in the outcome variable, yet simultaneously having statistically significant coefficients for some predictor variables. R-squared (R2) is a measure of a model's overall fit, indicating the proportion of variability in the outcome variable explained by the predictor variables. A low R2 indicates that the model doesn't account for much of the variation in the outcome. Statistical significance of a predictor variable refers to the evidence against the null hypothesis of \"no association\" between the predictor and the outcome variable. A low p-value (generally less than 0.05) provides strong evidence against the null hypothesis, implying a statistically significant relationship between the predictor and the outcome. Several factors can contribute to a low R2, including weak predictors, multicollinearity, and missing predictors. Addressing these issues involves incorporating additional predictor variables and mitigating multicollinearity. A low R2 may not be a major concern if the goal is prediction, but further refinement might be needed if understanding the specific effects of individual predictors is important. Model building is an iterative process that involves evaluating both overall fit and individual predictor contributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595c822b",
   "metadata": {},
   "source": [
    "# Post-Lecture HW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e54c83",
   "metadata": {},
   "source": [
    "# Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b95556",
   "metadata": {},
   "source": [
    "The first code cell demonstrates the creation of training and testing datasets using the train_test_split function from the sklearn.model_selection module. It replaces missing values in the \"Type 2\" column of the pokeaman DataFrame with \"None\" using the fillna method, sets a random seed for reproducibility, calculates fifty_fifty_split_size, and splits the data into two subsets: pokeaman_train (training dataset) and pokeaman_test (testing dataset). The train_size argument is set to fifty_fifty_split_size, ensuring half of the data is used for training.\n",
    "\n",
    "The purpose of splitting the data into training and testing datasets is to evaluate the performance and generalizability of a fitted model. The training dataset is used to train the model, learning patterns and relationships within the data, while the testing dataset assesses how well the model generalizes to new, unseen data. Comparing the model's performance on both datasets helps determine if it is overfitting (performing well on training data but poorly on testing data) or underfitting (performing poorly on both training and testing data). This process ensures selecting a model that is accurate and reliable for making predictions on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8af546",
   "metadata": {},
   "source": [
    "The second code demonstrates the process of fitting a multiple linear regression model using the statsmodels library in Python. It begins with creating a model specification (model_spec3) using smf.ols from the statsmodels.formula.api module. The formula 'HP ~ Attack + Defense' defines the linear form of the model as an additive model with two predictor variables. The model is trained on the pokeaman_train DataFrame, which is likely the training dataset created in the previous code cell.\n",
    "\n",
    "The model is fitted to the data using the.fit() method, which calculates the estimated regression coefficients. The summary() method is called on the fitted model (model3_fit), which generates a comprehensive summary of the regression results, including the estimated coefficients, their standard errors, p-values, R-squared, and other statistics.\n",
    "\n",
    "The output of model3_fit.summary() provides insights into the relationship between HP, Attack, and Defense in the pokeaman_train dataset. P-values indicate statistical evidence against the null hypothesis that the corresponding coefficient is zero, meaning there is no linear association between that predictor variable and HP \"on average.\" A higher R-squared value indicates a better fit of the model to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517870ba",
   "metadata": {},
   "source": [
    "The third code demonstrates how to generate predictions from a fitted model and calculate the \"out of sample\" R-squared to evaluate its performance on a testing dataset. This is a common practice in machine learning to assess the model's ability to generalize to new, unseen data. The code uses the predict() method to generate predictions for the outcome variable (HP) using predictor variables (Attack and Defense) from the pokeaman_test DataFrame. The results are stored in the yhat_model3 variable, representing the model's predictions for HP for each Pokemon in the testing dataset.\n",
    "\n",
    "The \"out of sample\" R-squared is a crucial metric for evaluating the model's generalizability. A high \"out of sample\" R-squared indicates that the model is generalizing well to new data, while a low \"out of sample\" R-squared suggests that the model may be overfitting to the training dataset. Comparing these two R-squared values can provide insights into the reliability of the model and its applicability to making predictions on new data. This process is essential for developing models that are not only accurate but also generalizable and reliable in real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5b778a",
   "metadata": {},
   "source": [
    "The fourth code demonstrates the process of fitting a multiple linear regression model with a complex linear form, which includes interactions between multiple predictor variables. It highlights the risks of using an overly complex model, particularly when dealing with limited data, which may lead to overfitting. The initial linear form of the model is defined as HP (Hit Points) and includes interactions between the following predictor variables: Attack, Defense, Speed, and Legendary. The code adds more complexity by including interactions with two additional predictor variables, \"Sp. Def\" and \"Sp. Atk\". However, the code warns against adding more variables, as it would create a massive number of interaction combinations, leading to computational issues.\n",
    "\n",
    "The model specification is created using smf.ols, and the model is trained on the pokeaman_train DataFrame. The model is fitted to the training data, calculating the estimated regression coefficients, and a summary of the regression results is generated. The code suggests that the model might overfit to the training data, capturing noise or spurious relationships that do not generalize well to unseen data. The warning in the code advises against including more variables that would further increase the model's complexity. This code snippet serves as a demonstration of the dangers of using an overly complex model, especially with limited data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdbe43c",
   "metadata": {},
   "source": [
    "The fifth code generates predictions from a fitted complex model (model4_fit) on the testing dataset (pokeaman_test) and calculates and compares the \"in sample\" and \"out of sample\" R-squared values. This is crucial for understanding how well the model generalizes to new, unseen data. The code uses the.predict() method to generate predictions for the outcome variable (HP) using predictor variables from the pokeaman_test DataFrame. The actual HP values are extracted from the pokeaman_test DataFrame and stored in the y variable. The \"in sample\" R-squared value is stored in the model4_fit object, representing the proportion of variation in HP explained by the model using the training dataset. The \"out of sample\" R-squared is calculated and printed, indicating the model's performance on unseen testing data. If the \"out of sample\" R-squared is significantly lower than the \"in sample\" R-squared, it confirms the suspicion of overfitting. A high \"out of sample\" R-squared, close to the \"in sample\" R-squared, suggests that the complex relationships captured in the training data also hold true in the testing data. However, caution is needed as this could be due to chance factors, especially if the datasets are small. The code focuses on evaluating the generalizability of a complex model suspected to be overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32ad55b",
   "metadata": {},
   "source": [
    "Summary:\n",
    "\n",
    "The discussion on R-squared, a measure of a model's goodness of fit, emphasized the importance of evaluating model performance on unseen data to assess its generalizability. Two code snippets were used to calculate and compare \"in sample\" and \"out of sample\" R-squared values. The first snippet used a simpler model, while the second used an overly complex model. The key point was to emphasize the importance of evaluating model performance on unseen data to assess its generalizability. A high \"out of sample\" R-squared suggests that the model can reliably predict outcomes for new data, while a low value indicates potential overfitting to the training data. The second code snippet focused on fitting a complex model, which included numerous interaction terms between predictor variables. The conversation highlighted the risk of overfitting associated with such models, especially when the dataset is limited. The discussion also highlighted the importance of R-squared comparison and the role of model complexity in influencing the risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f26184",
   "metadata": {},
   "source": [
    "# Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0d9207",
   "metadata": {},
   "source": [
    "The model4_linear_form is a complex model that creates new predictor variables by specifying interactions between existing variables. This results in a complex model that overfits to the training data and exhibits poor out-of-sample generalization due to multicollinearity in the design matrix model4_spec.exog. Multicollinearity occurs when predictor variables in the design matrix are highly correlated, making it difficult for the model to distinguish individual effects of correlated predictor variables on the outcome variable. This leads to unstable coefficient estimates and reduced statistical power. In the case of model4_fit, the multicollinearity observed in the design matrix contributes to its lack of \"out of sample\" generalization. The overfit model detects idiosyncratic associations in the training data that do not generalize to the testing data due to high multicollinearity. The model struggles to accurately estimate the true relationships between predictors and the outcome, leading to poor performance on unseen data. In essence, the complex model4_linear_form creates numerous highly correlated predictor variables in the design matrix, leading to multicollinearity, hindering the model's ability to accurately estimate the true effects of predictor variables, resulting in overfitting and poor generalization to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427cb9a2",
   "metadata": {},
   "source": [
    "Summary:\n",
    "\n",
    "The conversation explored how the model4_linear_form specification creates new predictor variables and the resulting multicollinearity in the design matrix (model4_spec.exog). The interaction clarified that the model4_linear_form creates new predictor variables by specifying interaction terms, which are products of two or more predictor variables. For example, an interaction between \"Attack\" and \"Speed\" would create a new variable, \"Attack:Speed,\" in the design matrix. Including many interaction terms increases model complexity.\n",
    "Multicollinearity arises when predictor variables are highly correlated, making it difficult to separate their individual effects on the outcome variable. The conversation noted that multicollinearity can be assessed using the condition number, with high values suggesting substantial multicollinearity. High multicollinearity leads to unstable coefficient estimates and reduced statistical power, hindering the model's ability to generalize to new data.\n",
    "In the case of model4_fit, the multicollinearity in the design matrix, as observed in np.corrcoef(model4_spec.exog), contributes to poor out-of-sample generalization. The overfit model detects spurious associations in the training data that do not generalize to the testing data. This is because a complex linear form specification can allow a model fit to identify idiosyncratic associations specific to the training dataset but not present in the testing dataset. Consequently, the model struggles to estimate the true relationships between predictors and the outcome variable, leading to poor performance on unseen data.\n",
    "The conversation highlighted the importance of considering model complexity and multicollinearity in multiple linear regression analysis. While interactions can capture complex relationships between variables, excessive interactions can lead to overfitting and poor generalization. Assessing multicollinearity and carefully selecting predictor variables are crucial steps in building robust and generalizable models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0fee09",
   "metadata": {},
   "source": [
    "# Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbf445a",
   "metadata": {},
   "source": [
    "The process of extending from model to model generally involves a systematic evaluation of model building, generalizability, multicollinearity, and model complexity, and analyzing the performance and complexity of previous models. The rational behind extending model3_fit and model4_fit into model5_linear_form might come from analyzing the performance and complexity of model3_fit and model4_fit, and realizing that model5_linear_form may have added more predictors or interactions to increase its predictive power, while model3_fit demonstrated restricted performance but strong generalizability. In contrast, model5_linear_form might have sought a more condensed specification by eliminating predictors or streamlining interactions if model4_fit was overfit and showed poor generalizability as a result of high multicollinearity. Simalarily, the rational behind extending model5_linear_form into model6_linear_form might be that based on the results of the generalizability evaluation and the evidence from the coefficient hypothesis testing, model6_linear_form might be an even better version of model5_linear_form. Based on the data, it may have been possible to eliminate predictors with statistically insignificant coefficients (high p-values) and add new interactions or variable transformations. Finally, the rational behind extending model6_linear_form into model7_linear_form could be that model7_linear_form might have been the result of exploring higher-order interactions, potentially building upon the structure of model6_linear_form. This extension might have been motivated by the desire to capture more complex relationships in the data, but it would have required careful consideration of multicollinearity and interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82e389e",
   "metadata": {},
   "source": [
    "Summary:\n",
    "\n",
    "Our conversation has focused on understanding the rationale and principles behind extending multiple linear regression models, specifically the development of model5_linear_form from model3_fit and model4_fit. We first discussed some key concepts from the sources:\n",
    "\n",
    "● Model building: The process of finding the best model for the data.\n",
    "\n",
    "● Generalizability: The ability of a model to accurately predict outcomes for new data.\n",
    "\n",
    "● Multicollinearity: Occurs when predictor variables are highly correlated.\n",
    "\n",
    "● Model Complexity: Refers to the number of predictor variables and interactions in the model.\n",
    "We then discussed how model5_linear_form might be developed based on those concepts:\n",
    "\n",
    "● Balancing Performance and Generalizability:\n",
    "\n",
    "○ If model3_fit demonstrated good generalizability but limited performance, model5_linear_form might have included more predictors or interactions from model3_fit to improve performance.\n",
    "\n",
    "○ Conversely, if model4_fit was overfit with poor generalizability due to excessive complexity or multicollinearity, model5_linear_form might have aimed for a simpler model by removing predictors or interactions from model4_fit.\n",
    "We also discussed some model building strategies:\n",
    "\n",
    "● Evidence-based model building: Techniques like forward and backward selection, which use hypothesis testing of coefficients to choose which predictor variables to include or exclude.\n",
    "\n",
    "● Performance-based model building: A framework that uses a train-test split to evaluate the predictive accuracy of a model on new data.\n",
    "We then went over some concepts relevant to model performance and complexity:\n",
    "\n",
    "● R-squared: Represents the proportion of variation in the outcome variable explained by the model.\n",
    "\n",
    "● Overfitting: Occurs when a model with many predictors fits the training data too well and doesn't generalize well to new data.\n",
    "\n",
    "● Parsimonious models: Simpler models that generalize better to new data.\n",
    "Finally, we explored multicollinearity:\n",
    "\n",
    "● Unreliable coefficient estimates: Can be caused by high correlation between predictor variables, making it difficult to isolate the effect of any one predictor on the outcome.\n",
    "\n",
    "● Condition number: A way to assess multicollinearity in the design matrix.\n",
    "\n",
    "● Centering and scaling: Techniques that can help reduce multicollinearity by standardizing the scales of continuous predictor variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5903d0",
   "metadata": {},
   "source": [
    "# Question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2829249",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m out_of_sample_r2 \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):  \u001b[38;5;66;03m# 100 iterations \u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m train_test_split(\u001b[43mdata\u001b[49m, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)  \u001b[38;5;66;03m# 50-50 split\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Fit the model on the training data\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     model \u001b[38;5;241m=\u001b[39m smf\u001b[38;5;241m.\u001b[39mols(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutcome_variable ~ predictor_variable_1 + predictor_variable_2\u001b[39m\u001b[38;5;124m'\u001b[39m, data\u001b[38;5;241m=\u001b[39mtrain)\u001b[38;5;241m.\u001b[39mfit()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This is where you would put a dataframe named 'data' with your outcome and predictor variables\n",
    "\n",
    "in_sample_r2 = []\n",
    "out_of_sample_r2 = []\n",
    "\n",
    "for i in range(100):  # 100 iterations \n",
    "    train, test = train_test_split(data, test_size=0.5)  # 50-50 split\n",
    "    \n",
    "    # Fit the model on the training data\n",
    "    model = smf.ols('outcome_variable ~ predictor_variable_1 + predictor_variable_2', data=train).fit()\n",
    "\n",
    "    # Calculate in-sample R-squared\n",
    "    in_sample_r2.append(np.corrcoef(train['outcome_variable'], model.fittedvalues)[1]**2)\n",
    "\n",
    "    # Predict on the testing data\n",
    "    predictions = model.predict(test)\n",
    "\n",
    "    # Calculate out-of-sample R-squared\n",
    "    out_of_sample_r2.append(np.corrcoef(test['outcome_variable'], predictions)[1]**2)\n",
    "\n",
    "# Visualize the results\n",
    "plt.scatter(in_sample_r2, out_of_sample_r2)\n",
    "plt.xlabel(\"In-Sample R-squared\")\n",
    "plt.ylabel(\"Out-of-Sample R-squared\")\n",
    "plt.title(\"In-Sample vs. Out-of-Sample Model Performance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce32bfdf",
   "metadata": {},
   "source": [
    "The code's purpose is to demonstrate the concept of model generalizability by repeatedly comparing \"in-sample\" and \"out-of-sample\" model performance. This helps determine whether the model is overfit to the training data and assess its predictive accuracy on unseen data. The for loop iterates 100 times, with each iteration involving random data splitting into a training and testing set using the train_test_split function. A multiple linear regression model is fitted to the training data using statsmodels.formula.api. The \"in-sample\" R-squared is calculated by finding the squared correlation between the observed outcomes in the training data and the model's fitted values. The resulting in-sample and out-of-sample R-squared values are plotted as a scatter plot. The scatter plot visually displays the relationship between in-sample and out-of-sample performance across different random data splits. Overfitting is indicated when many points lie significantly above the diagonal line, indicating that the in-sample performance is often much better than the out-of-sample performance. Variability is also demonstrated by the spread of points demonstrating the variability in model performance depending on the random split of the data. Key concepts and considerations include model generalization, overfitting vs. underfitting, train-test split, and model performance metrics. The scatter plot provides a visual tool to assess these concepts and underscores the importance of using robust model building practices to create models that predict accurately on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a959d95f",
   "metadata": {},
   "source": [
    "Summary:\n",
    "\n",
    "The conversation focused on understanding and demonstrating the concept of model generalizability by comparing “in-sample” and “out-of-sample” model performance. You requested a Python for loop to create, collect, and visualize these metrics across multiple iterations. The code provided in the response illustrated this concept using multiple linear regression and R-squared as the performance metric. The conversation then explored the interpretation of the generated scatter plot, emphasizing the identification of overfitting.\n",
    "The discussion highlighted several key concepts from the sources:\n",
    "\n",
    "● Importance of Model Generalization: Building models that perform well on new, unseen data is crucial. The sources emphasize evaluating out-of-sample performance as a means of assessing generalizability.\n",
    "\n",
    "● Overfitting vs. Underfitting: While the provided code focuses on identifying overfitting, the conversation acknowledges that underfitting is also a concern, as discussed in the sources.\n",
    "\n",
    "● Train-Test Split: The sources stress the need for a train-test split, and specifically recommend using the train_test_split function.\n",
    "\n",
    "● Model Performance Metrics: The conversation uses R-squared to measure model performance. It also references the sources' discussion of R-squared's calculation and its limitations as a single metric for model quality.\n",
    "The response also included information about the sklearn.model_selection library, from which the train_test_split function is imported. This library was not mentioned in your provided sources, so you may want to verify that information independently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f863f9cd",
   "metadata": {},
   "source": [
    "# Question 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17be9bec",
   "metadata": {},
   "source": [
    "The meaning of the illustration created by the four snippets of code is to demonstrate the concept of model generalizability. \n",
    "\n",
    "The first code snippet demonstrates model generalizability in multiple linear regression by comparing the performance of two models on two sets of data. Model generalizability is the ability of a fitted model to perform well on new, unseen data. In-sample model performance refers to the model's ability to predict outcomes for the data it was trained on, while out-of-sample performance refers to the model's performance on data it has not seen before. The code uses R-squared as a metric to evaluate model performance. If the model performs well on data from other generations, it suggests generalization, while a significant drop in performance on out-of-sample data suggests potential overfitting.\n",
    "\n",
    "The second code snippet demonstrates the concept of model generalizability by training a model using data from \"Generation 1\" to \"Generation 5\" and evaluating its performance on data from \"Generation 6\". A model trained on data from \"Generations 1-5\" that performs well on \"Generation 6\" data is considered generalizable. However, if there's a significant drop in performance on \"Generation 6\" data, it indicates potential overfitting. Overfitting means the model may have learned patterns specific to \"Generations 1-5\" that do not hold for \"Generation 6\".\n",
    "\n",
    "The third code snippet assesses model generalizability in multiple linear regression by comparing \"in-sample\" and \"out-of-sample\" model performance. The code defines and fits two models: model6_fit and model6_gen1_predict_future_fit. The R-squared values for these models are calculated using the correlation coefficient between the outcome and the fitted values. If the \"out-of-sample\" R-squared is significantly lower than the \"in-sample\" R-squared, it may suggest that the model is overfitting to the training data. A large difference between \"in-sample\" and \"out-of-sample\" model performance may also be a consequence of multicollinearity, which can make it difficult to discern individual effects of predictors and lead to unstable model estimates.\n",
    "\n",
    "The code demonstrates the importance of model generalizability in multiple linear regression, focusing on the model's ability to accurately predict outcomes for Pokemon in \"Generation 6\" based on training data from \"Generations 1-5.\" It evaluates model performance using metrics like R-squared and a Train-test split. The code aims to assess how well a model trained on data from Pokemon \"Generations 1 to 5\" can predict outcomes for Pokemon in \"Generation 6\". It calculates and prints the \"in-sample\" R-squared values for both models using the data they were trained on. The \"out-of-sample\" R-squared values are compared to assess the generalizability of model6_gen1to5_predict_future_fit. The code demonstrates the context of new data becoming available sequentially, such as in time-series data or datasets that evolve over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c3be42",
   "metadata": {},
   "source": [
    "Summary:\n",
    "\n",
    "The conversation revolved around assessing model generalizability, which refers to a model's ability to predict outcomes on unseen data. The code used a model (model6_gen1to5_predict_future_fit) trained on data from Pokemon from \"Generations 1 to 5\" and tested its predictive accuracy on data from \"Generation 6\" Pokemon. The code calculated and printed the \"in-sample\" and \"out-of-sample\" R-squared values to assess whether the model was overfitting to the training data. Key concepts highlighted include model generalizability, model performance, and train-test split. Additional points to consider include model complexity, multicollinearity, and model diagnostics. Complex models may be more prone to overfitting, capturing noise and patterns specific to the training data that do not generalize well to new data. Highly correlated predictor variables can lead to unstable models, making it challenging to determine individual effects of predictors. Assessing the assumptions of a multiple linear regression model is essential to ensure its validity.\n",
    "\n",
    "Key Concepts Highlighted:\n",
    "\n",
    "● Model Generalizability: This refers to a model's ability to perform well on unseen data. It's an essential concept in statistical modeling as it ensures the model's reliability and predictive accuracy in real-world scenarios.\n",
    "\n",
    "● Model Performance: Metrics like R-squared can be used to evaluate a model's performance. R-squared measures the proportion of variation in the outcome variable explained by the model.\n",
    "\n",
    "● Train-test Split: This technique divides a dataset into separate sets for training and testing, allowing for an assessment of a model's generalizability.\n",
    "Additional Points to Consider:\n",
    "\n",
    "● Model Complexity: More complex models may be more prone to overfitting, capturing noise and patterns specific to the training data that do not generalize well to new data.\n",
    "\n",
    "● Multicollinearity: Highly correlated predictor variables can lead to unstable models and make it challenging to determine the individual effects of predictors.\n",
    "\n",
    "● Model Diagnostics: Assessing the assumptions of the multiple linear regression model, such as normality, homoskedasticity, and linear form, is essential to ensure the model's validity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
